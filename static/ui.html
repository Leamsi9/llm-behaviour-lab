<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Harness Lab - Compare Base vs RLHF Models</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/htmx.org@1.9.6"></script>
    <script src="https://unpkg.com/htmx.org/dist/ext/ws.js"></script>
    <style>
        .split-pane {
            display: flex;
            flex-direction: row;
            height: calc(100vh - 200px);
        }
        .pane {
            flex: 1;
            padding: 1rem;
            overflow-y: auto;
            border: 1px solid #e2e8f0;
            margin: 0.5rem;
            border-radius: 0.5rem;
        }
        .model-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid #e2e8f0;
        }
        .model-output {
            min-height: 200px;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            background: #f8fafc;
            padding: 1rem;
            border-radius: 0.25rem;
            margin-top: 1rem;
        }
        .token-counter {
            font-size: 0.875rem;
            color: #64748b;
        }
        .latency {
            font-size: 0.875rem;
            color: #64748b;
            margin-left: 1rem;
        }
        .tabs {
            display: flex;
            margin-bottom: 1rem;
        }
        .tab {
            padding: 0.5rem 1rem;
            cursor: pointer;
            border: 1px solid #e2e8f0;
            background: #f8fafc;
            margin-right: 0.5rem;
            border-radius: 0.25rem 0.25rem 0 0;
        }
        .tab.active {
            background: #fff;
            border-bottom: 2px solid #3b82f6;
        }
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }
    </style>
</head>
<body class="bg-gray-50 p-4">
    <div class="max-w-7xl mx-auto">
        <div class="flex justify-between items-center mb-6">
            <h1 class="text-2xl font-bold text-gray-800">LLM Harness Lab - Compare Base vs RLHF Models</h1>
            <div id="connectionStatus" class="text-sm px-3 py-1 rounded-full bg-gray-200 text-gray-600">
                <span class="inline-block w-2 h-2 rounded-full bg-gray-400 mr-2"></span>
                Connecting...
            </div>
        </div>
        
        <div class="bg-white rounded-lg shadow p-6 mb-6">
            <div class="mb-4">
                <label class="block text-sm font-medium text-gray-700 mb-2">System Prompt</label>
                <textarea id="systemPrompt" class="w-full p-2 border rounded-md h-20" placeholder="Enter system prompt...">You are a helpful AI assistant. Think step by step and be concise in your responses.</textarea>
            </div>
            
            <div id="modelInfo" class="mb-4 p-4 bg-blue-50 rounded-md">
                <h3 class="font-medium text-blue-800 mb-3">Select Models to Compare</h3>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                    <div>
                        <label class="block text-sm font-medium text-blue-800 mb-1">Left Side (Base/Model 1)</label>
                        <select id="baseModelSelect" class="w-full p-2 border border-blue-200 rounded-md text-sm bg-white">
                            <option value="">Loading models...</option>
                        </select>
                    </div>
                    <div>
                        <label class="block text-sm font-medium text-blue-800 mb-1">Right Side (Instruct/Model 2)</label>
                        <select id="instructModelSelect" class="w-full p-2 border border-blue-200 rounded-md text-sm bg-white">
                            <option value="">Loading models...</option>
                        </select>
                    </div>
                </div>
                <p class="text-xs text-blue-600 mt-2">
                    ðŸ’¡ Tip: Compare base vs instruct, or any two models side-by-side
                </p>
            </div>
            
            <div class="mb-4">
                <label class="block text-sm font-medium text-gray-700 mb-2">User Input</label>
                <textarea id="userInput" class="w-full p-2 border rounded-md h-32" placeholder="Enter your message...">Explain the difference between the base and instruction-tuned versions of Llama 3 in simple terms.</textarea>
            </div>
            
            <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-4">
                <div>
                    <label class="block text-sm font-medium text-gray-700 mb-1">Temperature</label>
                    <input type="range" id="temperature" min="0" max="2" step="0.1" value="0.7" class="w-full">
                    <div class="text-sm text-gray-600 text-center"><span id="tempValue">0.7</span></div>
                </div>
                <div>
                    <label class="block text-sm font-medium text-gray-700 mb-1">Max Tokens</label>
                    <input type="number" id="maxTokens" min="1" max="4096" value="1024" class="w-full p-2 border rounded-md">
                </div>
                <div class="flex items-end gap-2">
                    <button id="generateBtn" class="bg-blue-600 text-white px-4 py-2 rounded-md hover:bg-blue-700 flex-1">
                        Generate
                    </button>
                    <button id="stopBtn" class="bg-red-600 text-white px-4 py-2 rounded-md hover:bg-red-700 flex-1 hidden" disabled>
                        Stop
                    </button>
                </div>
            </div>
            
            <div class="flex items-center text-sm text-gray-600">
                <div class="mr-4">
                    <input type="checkbox" id="compareMode" class="mr-2" checked>
                    <label for="compareMode">Compare Base vs RLHF</label>
                </div>
                <div id="modelStatus" class="text-sm text-gray-500">
                    Models: Loading...
                </div>
            </div>
        </div>
        
        <div id="outputContainer" class="split-pane">
            <!-- Base Model Pane -->
            <div id="basePane" class="pane">
                <div class="model-header">
                    <div>
                        <h3 class="font-bold">Base Model</h3>
                        <p class="text-xs text-gray-500 mt-1" id="baseModelDisplay">Loading...</p>
                    </div>
                    <div class="flex items-center">
                        <span id="baseTokenCount" class="token-counter">0 tokens</span>
                        <span id="baseLatency" class="latency">-</span>
                    </div>
                </div>
                <div id="baseOutput" class="model-output"></div>
            </div>
            
            <!-- RLHF Model Pane -->
            <div id="rlhfPane" class="pane">
                <div class="model-header">
                    <div>
                        <h3 class="font-bold">RLHF Model</h3>
                        <p class="text-xs text-gray-500 mt-1" id="rlhfModelDisplay">Loading...</p>
                    </div>
                    <div class="flex items-center">
                        <span id="rlhfTokenCount" class="token-counter">0 tokens</span>
                        <span id="rlhfLatency" class="latency">-</span>
                    </div>
                </div>
                <div id="rlhfOutput" class="model-output"></div>
            </div>
        </div>
        
        <div class="mt-6 text-sm text-gray-500">
            <p>Note: Make sure Ollama is running and models are pulled.</p>
            <p>Pull models: <code>ollama pull qwen2.5:7b</code> and <code>ollama pull qwen2.5:7b-base</code></p>
        </div>
    </div>

    <script>
        // DOM Elements
        const systemPrompt = document.getElementById('systemPrompt');
        const userInput = document.getElementById('userInput');
        const temperature = document.getElementById('temperature');
        const tempValue = document.getElementById('tempValue');
        const maxTokens = document.getElementById('maxTokens');
        const compareMode = document.getElementById('compareMode');
        const generateBtn = document.getElementById('generateBtn');
        const stopBtn = document.getElementById('stopBtn');
        const modelStatus = document.getElementById('modelStatus');
        const connectionStatus = document.getElementById('connectionStatus');
        const baseModelSelect = document.getElementById('baseModelSelect');
        const instructModelSelect = document.getElementById('instructModelSelect');
        const baseModelDisplay = document.getElementById('baseModelDisplay');
        const rlhfModelDisplay = document.getElementById('rlhfModelDisplay');
        
        // Output panes
        const basePane = document.getElementById('basePane');
        const rlhfPane = document.getElementById('rlhfPane');
        const baseOutput = document.getElementById('baseOutput');
        const rlhfOutput = document.getElementById('rlhfOutput');
        const baseTokenCount = document.getElementById('baseTokenCount');
        const rlhfTokenCount = document.getElementById('rlhfTokenCount');
        const baseLatency = document.getElementById('baseLatency');
        const rlhfLatency = document.getElementById('rlhfLatency');
        
        // Store model names
        let currentBaseModel = '';
        let currentInstructModel = '';
        let availableModels = [];
        let isGenerating = false;
        let baseActive = false;
        let rlhfActive = false;
        let basePromptTokens = 0;
        let baseCompletionTokens = 0;
        let rlhfPromptTokens = 0;
        let rlhfCompletionTokens = 0;
        
        // Update temperature value display
        temperature.addEventListener('input', () => {
            tempValue.textContent = temperature.value;
        });
        
        // Toggle comparison mode
        compareMode.addEventListener('change', () => {
            if (compareMode.checked) {
                rlhfPane.style.display = 'block';
                basePane.style.flex = '1';
            } else {
                rlhfPane.style.display = 'none';
                basePane.style.flex = '1';
            }
        });
        
        // WebSocket connections
        let baseWs, rlhfWs;
        let baseStartTime, rlhfStartTime;
        let baseTokens = 0, rlhfTokens = 0;
        
        function connectWebSockets() {
            // Close existing connections if any
            if (baseWs) baseWs.close();
            if (rlhfWs) rlhfWs.close();
            
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsBaseUrl = `${protocol}//${window.location.host}/ws`;
            
            // Base model connection
            baseWs = new WebSocket(wsBaseUrl);
            baseWs.onmessage = (event) => handleMessage(event, 'base');
            baseWs.onopen = () => updateModelStatus();
            baseWs.onclose = () => updateModelStatus();
            
            // RLHF model connection
            rlhfWs = new WebSocket(wsBaseUrl);
            rlhfWs.onmessage = (event) => handleMessage(event, 'rlhf');
            rlhfWs.onopen = () => updateModelStatus();
            rlhfWs.onclose = () => updateModelStatus();
        }
        
        async function loadAvailableModels() {
            try {
                const response = await fetch('/api/models');
                const data = await response.json();
                
                availableModels = data.models || [];
                
                // Populate dropdowns
                baseModelSelect.innerHTML = '';
                instructModelSelect.innerHTML = '';
                
                if (availableModels.length === 0) {
                    baseModelSelect.innerHTML = '<option value="">No models found - Run: ollama pull MODEL</option>';
                    instructModelSelect.innerHTML = '<option value="">No models found - Run: ollama pull MODEL</option>';
                    return;
                }
                
                availableModels.forEach(model => {
                    const option1 = document.createElement('option');
                    option1.value = model;
                    option1.textContent = model;
                    baseModelSelect.appendChild(option1);
                    
                    const option2 = document.createElement('option');
                    option2.value = model;
                    option2.textContent = model;
                    instructModelSelect.appendChild(option2);
                });
                
                // Set default selections
                if (data.current) {
                    baseModelSelect.value = data.current.base || availableModels[0];
                    instructModelSelect.value = data.current.instruct || availableModels[Math.min(1, availableModels.length - 1)];
                }
                
                // Update current models
                updateSelectedModels();
                
            } catch (error) {
                console.error('Failed to load models:', error);
                baseModelSelect.innerHTML = '<option value="">Error loading models</option>';
                instructModelSelect.innerHTML = '<option value="">Error loading models</option>';
            }
        }
        
        function updateSelectedModels() {
            currentBaseModel = baseModelSelect.value;
            currentInstructModel = instructModelSelect.value;
            
            baseModelDisplay.textContent = `Using: ${currentBaseModel || 'None'}`;
            rlhfModelDisplay.textContent = `Using: ${currentInstructModel || 'None'}`;
        }
        
        // Listen for model selection changes
        baseModelSelect.addEventListener('change', updateSelectedModels);
        instructModelSelect.addEventListener('change', updateSelectedModels);
        
        async function fetchModelInfo() {
            try {
                const response = await fetch('/api/health');
                const data = await response.json();
                
                if (data.ollama) {
                    connectionStatus.innerHTML = '<span class="inline-block w-2 h-2 rounded-full bg-green-500 mr-2"></span>Connected to Ollama';
                    connectionStatus.className = 'text-sm px-3 py-1 rounded-full bg-green-100 text-green-700';
                } else {
                    connectionStatus.innerHTML = '<span class="inline-block w-2 h-2 rounded-full bg-red-500 mr-2"></span>Ollama not connected';
                    connectionStatus.className = 'text-sm px-3 py-1 rounded-full bg-red-100 text-red-700';
                    
                    baseModelDisplay.textContent = 'Ollama not connected - Run: ollama serve';
                    rlhfModelDisplay.textContent = 'Ollama not connected - Run: ollama serve';
                }
            } catch (error) {
                connectionStatus.innerHTML = '<span class="inline-block w-2 h-2 rounded-full bg-red-500 mr-2"></span>Connection error';
                connectionStatus.className = 'text-sm px-3 py-1 rounded-full bg-red-100 text-red-700';
                
                baseModelDisplay.textContent = 'Cannot reach server';
                rlhfModelDisplay.textContent = 'Cannot reach server';
            }
        }
        
        async function updateModelStatus() {
            const baseReady = baseWs && baseWs.readyState === WebSocket.OPEN;
            const rlhfReady = rlhfWs && rlhfWs.readyState === WebSocket.OPEN;
            
            // Check Ollama connection first
            let ollamaConnected = false;
            try {
                const response = await fetch('/api/health');
                const data = await response.json();
                ollamaConnected = data.ollama;
            } catch (error) {
                ollamaConnected = false;
            }
            
            if (!ollamaConnected) {
                modelStatus.textContent = 'Models: Ollama not connected';
                modelStatus.style.color = 'red';
                generateBtn.disabled = true;
            } else if (baseReady && rlhfReady) {
                modelStatus.textContent = 'Models: Both loaded and ready';
                modelStatus.style.color = 'green';
                generateBtn.disabled = false;
            } else if (baseReady) {
                modelStatus.textContent = 'Models: Only base model loaded';
                modelStatus.style.color = 'orange';
                generateBtn.disabled = false;
            } else if (rlhfReady) {
                modelStatus.textContent = 'Models: Only RLHF model loaded';
                modelStatus.style.color = 'orange';
                generateBtn.disabled = false;
                compareMode.checked = false;
                compareMode.dispatchEvent(new Event('change'));
            } else {
                modelStatus.textContent = 'Models: Connecting...';
                modelStatus.style.color = 'red';
                generateBtn.disabled = true;
            }
            
            // Also fetch model info
            await fetchModelInfo();
        }
        
        function setGeneratingState(active) {
            isGenerating = active;
            if (active) {
                stopBtn.classList.remove('hidden');
                stopBtn.disabled = false;
                generateBtn.textContent = 'Generate';
            } else {
                stopBtn.classList.add('hidden');
                stopBtn.disabled = true;
            }
        }

        function updateTokenDisplay(modelType, promptTokens, completionTokens, cancelled = false) {
            const tokenElement = modelType === 'base' ? baseTokenCount : rlhfTokenCount;
            if (cancelled) {
                tokenElement.textContent = 'Cancelled';
                return;
            }
            const total = promptTokens + completionTokens;
            tokenElement.textContent = `prompt ${promptTokens} â€¢ output ${completionTokens} (total ${total})`;
        }

        function handleModelFinished(modelType, metrics, cancelled = false) {
            const latencyElement = modelType === 'base' ? baseLatency : rlhfLatency;

            if (cancelled) {
                latencyElement.textContent = 'cancelled';
                updateTokenDisplay(modelType, 0, 0, true);
            } else if (metrics) {
                const promptTokens = metrics.prompt_tokens || 0;
                const completionTokens = metrics.completion_tokens || 0;
                const latency = metrics.latency || 0;
                const tps = metrics.tokens_per_second ? metrics.tokens_per_second.toFixed(1) : '0.0';

                if (modelType === 'base') {
                    basePromptTokens = promptTokens;
                    baseCompletionTokens = completionTokens;
                } else {
                    rlhfPromptTokens = promptTokens;
                    rlhfCompletionTokens = completionTokens;
                }

                updateTokenDisplay(modelType, promptTokens, completionTokens);
                latencyElement.textContent = `${latency.toFixed(2)}s (${tps} t/s)`;
            }

            if (modelType === 'base') baseActive = false;
            if (modelType === 'rlhf') rlhfActive = false;

            if (!baseActive && !rlhfActive) {
                setGeneratingState(false);
            }
        }

        async function cancelGeneration(target = 'both') {
            const sendCancel = (ws, modelKey) => {
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ command: 'cancel', model_key: modelKey }));
                }
            };

            if (target === 'base' || target === 'both') {
                sendCancel(baseWs, 'base');
            }
            if (target === 'rlhf' || target === 'both') {
                sendCancel(rlhfWs, 'instruct');
            }

            await new Promise(resolve => setTimeout(resolve, 20));
        }

        function handleMessage(event, modelType) {
            const data = JSON.parse(event.data);
            const outputElement = modelType === 'base' ? baseOutput : rlhfOutput;
            const tokenElement = modelType === 'base' ? baseTokenCount : rlhfTokenCount;
            const latencyElement = modelType === 'base' ? baseLatency : rlhfLatency;
            
            if (data.error) {
                outputElement.textContent += `\nError: ${data.error}\n`;
                handleModelFinished(modelType, null, true);
                return;
            }
            
            if (data.token === '[DONE]' || data.token === '[CANCELLED]') {
                handleModelFinished(modelType, data.metrics, data.cancelled || data.token === '[CANCELLED]');
                return;
            }
            
            // Append the token to the output
            outputElement.textContent += data.token;
            outputElement.scrollTop = outputElement.scrollHeight;
        }

        // Generate button click handler
        generateBtn.addEventListener('click', async () => {
            if (!userInput.value.trim()) {
                alert('Please enter a message');
                return;
            }
            
            if (!currentBaseModel && !currentInstructModel) {
                alert('Please select at least one model to generate with.');
                return;
            }

            await cancelGeneration();
            
            // Clear previous outputs
            baseOutput.textContent = '';
            rlhfOutput.textContent = '';
            updateTokenDisplay('base', 0, 0);
            updateTokenDisplay('rlhf', 0, 0);
            baseLatency.textContent = '-';
            rlhfLatency.textContent = '-';
            
            // Prepare the payload
            const payload = {
                system: systemPrompt.value,
                user: userInput.value,
                template: '{% for message in messages %}{{message.role|upper}}: {{message.content}}\n{% endfor %}ASSISTANT:',
                temp: parseFloat(temperature.value),
                max_tokens: parseInt(maxTokens.value) || 1024,
                stop: ['USER:', 'ASSISTANT:', '</s>']
            };
            
            baseActive = false;
            rlhfActive = false;

            // Send to base model
            if (baseWs && baseWs.readyState === WebSocket.OPEN && currentBaseModel) {
                baseStartTime = Date.now();
                baseWs.send(JSON.stringify({ 
                    ...payload, 
                    use_base_model: true,
                    model_name: currentBaseModel 
                }));
                baseActive = true;
            }
            
            // Send to RLHF model if in compare mode
            if (compareMode.checked && rlhfWs && rlhfWs.readyState === WebSocket.OPEN && currentInstructModel) {
                rlhfStartTime = Date.now();
                rlhfWs.send(JSON.stringify({ 
                    ...payload, 
                    use_base_model: false,
                    model_name: currentInstructModel 
                }));
                rlhfActive = true;
            }

            if (!baseActive && !rlhfActive) {
                alert('No models are ready to generate. Check your connections.');
                return;
            }

            setGeneratingState(true);
        });

        stopBtn.addEventListener('click', async () => {
            await cancelGeneration();
        });
        
        // Initialize WebSocket connections when the page loads
        document.addEventListener('DOMContentLoaded', async () => {
            // Load available models first
            await loadAvailableModels();
            
            // Then connect WebSockets
            connectWebSockets();
            
            // Handle page visibility changes to reconnect if needed
            document.addEventListener('visibilitychange', () => {
                if (!document.hidden) {
                    connectWebSockets();
                }
            });
        });
        
        // Auto-resize textareas
        function autoResize(textarea) {
            textarea.style.height = 'auto';
            textarea.style.height = (textarea.scrollHeight) + 'px';
        }
        
        systemPrompt.addEventListener('input', () => autoResize(systemPrompt));
        userInput.addEventListener('input', () => autoResize(userInput));
        
        // Initial resize
        autoResize(systemPrompt);
        autoResize(userInput);
    </script>
</body>
</html>
